{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "3 - (CuratorNet) Training procedure.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ialab-puc/VisualRecSys-Tutorial-IUI2021/blob/curatornet-colab-link/3%20(CuratorNet)%20Training%20procedure.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-26T01:41:47.880863Z",
          "start_time": "2021-03-26T01:41:46.868570Z"
        },
        "id": "MII7MKGnoIph"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
        "\n",
        "from datasets.profile_mode import ProfileModeDataset\n",
        "from models.curatornet import CuratorNet\n",
        "from utils.curatornet_sampler import SameProfileSizeBatchSampler\n",
        "from trainers import Trainer\n",
        "from utils.data import extract_embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-26T01:41:50.055518Z",
          "start_time": "2021-03-26T01:41:50.052454Z"
        },
        "id": "SFW1NAEwoIpo"
      },
      "source": [
        "# Parameters\n",
        "RNG_SEED = 0\n",
        "EMBEDDING_PATH = os.path.join(\"data\", \"embedding-resnet50.npy\")\n",
        "TRAINING_PATH = os.path.join(\"data\", \"naive-profile-train.csv\")\n",
        "VALIDATION_PATH = os.path.join(\"data\", \"naive-profile-validation.csv\")\n",
        "CHECKPOINTS_DIR = os.path.join(\"checkpoints\")\n",
        "USE_GPU = True\n",
        "\n",
        "# Parameters (training)\n",
        "SETTINGS = {\n",
        "    \"batch_sampler:batch_size\": 128,\n",
        "    \"batch_sampler:profile_items_per_batch\": 60_000,\n",
        "    \"dataloader:num_workers\": os.cpu_count(),\n",
        "    \"dataloader:pin_memory\": True,\n",
        "    \"optimizer:lr\": 0.0001,\n",
        "    \"optimizer:weight_decay\": 0.0001,\n",
        "    \"scheduler:factor\": 0.6,\n",
        "    \"scheduler:patience\": 2,\n",
        "    \"scheduler:threshold\": 1e-4,\n",
        "    \"train:max_epochs\": 10,\n",
        "    \"train:max_lrs\": 10,\n",
        "    \"train:non_blocking\": True,\n",
        "    \"train:train_per_valid_times\": 1,\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-26T01:41:59.213466Z",
          "start_time": "2021-03-26T01:41:51.073361Z"
        },
        "id": "p1fZcAD1oIpp"
      },
      "source": [
        "%%time\n",
        "# Freezing RNG seed if needed\n",
        "if RNG_SEED is not None:\n",
        "    print(f\"\\nUsing random seed...\")\n",
        "    random.seed(RNG_SEED)\n",
        "    torch.manual_seed(RNG_SEED)\n",
        "    np.random.seed(RNG_SEED)\n",
        "\n",
        "# Load embedding from file\n",
        "print(f\"\\nLoading embedding from file... ({EMBEDDING_PATH})\")\n",
        "embedding = np.load(EMBEDDING_PATH, allow_pickle=True)\n",
        "\n",
        "# Extract features and \"id2index\" mapping\n",
        "print(\"\\nExtracting data into variables...\")\n",
        "embedding, _, _ = extract_embedding(embedding, verbose=True)\n",
        "print(f\">> Features shape: {embedding.shape}\")\n",
        "\n",
        "# DataLoaders initialization\n",
        "print(\"\\nInitialize DataLoaders\")\n",
        "# Training DataLoader\n",
        "train_dataset = ProfileModeDataset(\n",
        "    csv_file=TRAINING_PATH,\n",
        ")\n",
        "print(f\">> Training dataset: {len(train_dataset)}\")\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_batch_sampler = SameProfileSizeBatchSampler(\n",
        "    sampler=train_sampler,\n",
        "    batch_size=SETTINGS[\"batch_sampler:batch_size\"],\n",
        "    profile_items_per_batch=SETTINGS[\"batch_sampler:profile_items_per_batch\"],\n",
        ")\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    sampler=train_batch_sampler,\n",
        "    num_workers=SETTINGS[\"dataloader:num_workers\"],\n",
        "    pin_memory=SETTINGS[\"dataloader:pin_memory\"],\n",
        ")\n",
        "print(f\">> Training dataloader: {len(train_dataloader)}\")\n",
        "# Validation DataLoader\n",
        "valid_dataset = ProfileModeDataset(\n",
        "    csv_file=VALIDATION_PATH,\n",
        ")\n",
        "print(f\">> Validation dataset: {len(valid_dataset)}\")\n",
        "valid_sampler = SequentialSampler(valid_dataset)\n",
        "valid_batch_sampler = SameProfileSizeBatchSampler(\n",
        "    sampler=valid_sampler,\n",
        "    batch_size=SETTINGS[\"batch_sampler:batch_size\"],\n",
        "    profile_items_per_batch=SETTINGS[\"batch_sampler:profile_items_per_batch\"],\n",
        ")\n",
        "valid_dataloader = DataLoader(\n",
        "    valid_dataset,\n",
        "    sampler=valid_batch_sampler,\n",
        "    num_workers=SETTINGS[\"dataloader:num_workers\"],\n",
        "    pin_memory=SETTINGS[\"dataloader:pin_memory\"],\n",
        ")\n",
        "print(f\">> Validation dataloader: {len(valid_dataloader)}\")\n",
        "# Model initialization\n",
        "print(\"\\nInitialize model\")\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() and USE_GPU else \"cpu\")\n",
        "if torch.cuda.is_available() != USE_GPU:\n",
        "    print((f\"\\nNotice: Not using GPU - \"\n",
        "           f\"Cuda available ({torch.cuda.is_available()}) \"\n",
        "           f\"does not match USE_GPU ({USE_GPU})\"\n",
        "    ))\n",
        "model = CuratorNet(\n",
        "    torch.Tensor(embedding),\n",
        "    input_size=embedding.shape[1],\n",
        ").to(device)\n",
        "\n",
        "# Training setup\n",
        "print(\"\\nSetting up training\")\n",
        "optimizer = optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=SETTINGS[\"optimizer:lr\"],\n",
        "    weight_decay=SETTINGS[\"optimizer:weight_decay\"],\n",
        ")\n",
        "criterion = nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode=\"max\", factor=SETTINGS[\"scheduler:factor\"],\n",
        "    patience=SETTINGS[\"scheduler:patience\"], verbose=True,\n",
        "    threshold=SETTINGS[\"scheduler:threshold\"],\n",
        ")\n",
        "\n",
        "# Training\n",
        "print(\"\\nTraining\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-03-26T02:07:15.586369Z",
          "start_time": "2021-03-26T01:42:00.462845Z"
        },
        "id": "p2DO_DeOoIpq"
      },
      "source": [
        "%%time\n",
        "# Training\n",
        "version = (\n",
        "    f\"{model.__class__.__name__}_\"\n",
        "    f\"wikimedia\"\n",
        "    # f\"_resnet50_\"\n",
        "    # f\"{time.strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model, device, criterion, optimizer, scheduler,\n",
        "    checkpoint_dir=CHECKPOINTS_DIR,\n",
        "    version=version,\n",
        ")\n",
        "best_model, best_acc, best_loss, best_epoch = trainer.run(\n",
        "    SETTINGS[\"train:max_epochs\"], SETTINGS[\"train:max_lrs\"],\n",
        "    {\"train\": train_dataloader, \"validation\": valid_dataloader},\n",
        "    train_valid_loops=SETTINGS[\"train:train_per_valid_times\"],\n",
        ")\n",
        "\n",
        "# Final result\n",
        "print(f\"\\nBest ACC {best_acc} reached at epoch {best_epoch}\")\n",
        "print(best_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-HzZ0N3oIpr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
