{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from utils.data import extract_embedding, get_interactions_dataframe\n",
    "from utils.hashing import pre_hash, HashesContainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triplet sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "DATASET = \"Wikimedia\"\n",
    "assert DATASET in [\"Wikimedia\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode\n",
    "# Use 'MODE_PROFILE = True' for CuratorNet-like training \n",
    "# Use 'MODE_PROFILE = False' for VBPR-like training\n",
    "MODE_PROFILE = False\n",
    "MODE_PROFILE_VERBOSE = \"profile\" if MODE_PROFILE else \"user\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extractor\n",
    "FEATURE_EXTRACTOR = \"resnet50\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths (general)\n",
    "EMBEDDING_PATH = os.path.join(\"data\", DATASET, f\"embedding-{FEATURE_EXTRACTOR}.npy\")\n",
    "INTERACTIONS_PATH = os.path.join(\"data\", DATASET, f\"{DATASET.lower()}.csv\")\n",
    "OUTPUT_TRAIN_PATH = os.path.join(\"data\", DATASET, f\"naive-{MODE_PROFILE_VERBOSE}-train.csv\")\n",
    "OUTPUT_VALID_PATH = os.path.join(\"data\", DATASET, f\"naive-{MODE_PROFILE_VERBOSE}-validation.csv\")\n",
    "OUTPUT_EVAL_PATH = os.path.join(\"data\", DATASET, f\"naive-{MODE_PROFILE_VERBOSE}-evaluation.csv\")\n",
    "\n",
    "# General constants\n",
    "RNG_SEED = 0\n",
    "\n",
    "# Sampling constants\n",
    "GROUP_USER_INTERACTIONS_BY_TIMESTAMP = True\n",
    "MAX_PROFILE_SIZE = 10\n",
    "TOTAL_SAMPLES_TRAIN = 5_000_000\n",
    "TOTAL_SAMPLES_VALID = 500_000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freezing RNG seed if needed\n",
    "if RNG_SEED is not None:\n",
    "    print(f\"\\nUsing random seed... ({RNG_SEED})\")\n",
    "    random.seed(RNG_SEED)\n",
    "    np.random.seed(RNG_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding from file\n",
    "print(f\"\\nLoading embedding from file... ({EMBEDDING_PATH})\")\n",
    "embedding = np.load(EMBEDDING_PATH, allow_pickle=True)\n",
    "\n",
    "# Extract features and \"id2index\" mapping\n",
    "print(\"\\nExtracting data into variables...\")\n",
    "features, item_id2index, _ = extract_embedding(embedding, verbose=True)\n",
    "print(f\">> Features shape: {features.shape}\")\n",
    "del embedding  # Release some memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load interactions CSVs\n",
    "print(f\"\\nLoading interactions from files...\")\n",
    "interactions_df = get_interactions_dataframe(\n",
    "    INTERACTIONS_PATH,\n",
    "    display_stats=True,\n",
    ")\n",
    "\n",
    "# Apply 'item_id2index', to work with indexes only\n",
    "print(\"\\nApply 'item_id2index' mapping for items...\")\n",
    "interactions_df[\"item_id\"] = interactions_df[\"item_id\"].map(str)\n",
    "n_missing_ids = interactions_df[~interactions_df[\"item_id\"].isin(item_id2index)][\"item_id\"].count()\n",
    "interactions_df = interactions_df[interactions_df[\"item_id\"].isin(item_id2index)]\n",
    "interactions_df[\"item_id\"] = interactions_df[\"item_id\"].map(item_id2index)\n",
    "print(f\">> Mapping applied, ({n_missing_ids} values in 'item_id2index')\")\n",
    "\n",
    "# Store mapping from user_id to index (0-index, no skipping)\n",
    "print(\"\\nCreate 'user_id2index' mapping for users...\")\n",
    "unique_user_ids = interactions_df[\"user_id\"].unique()\n",
    "new_user_ids = np.argsort(unique_user_ids)\n",
    "user_id2index = dict(zip(unique_user_ids, new_user_ids))\n",
    "\n",
    "# Apply 'user_id2index', to work with indexes only\n",
    "print(\"\\nApply 'user_id2index' mapping for users...\")\n",
    "n_missing_ids = interactions_df[~interactions_df[\"user_id\"].isin(user_id2index)][\"user_id\"].count()\n",
    "interactions_df = interactions_df[interactions_df[\"user_id\"].isin(user_id2index)]\n",
    "interactions_df[\"user_id\"] = interactions_df[\"user_id\"].map(user_id2index)\n",
    "print(f\">> Mapping applied, ({n_missing_ids} values in 'user_id2index')\")\n",
    "\n",
    "# Split interactions data according to evaluation column\n",
    "evaluation_df = interactions_df[interactions_df[\"evaluation\"]]\n",
    "interactions_df = interactions_df[~interactions_df[\"evaluation\"]]\n",
    "assert not interactions_df.empty\n",
    "assert not evaluation_df.empty\n",
    "print(f\">> Evaluation: {evaluation_df.shape} | Interactions: {interactions_df.shape}\")\n",
    "\n",
    "# Form interactions baskets, grouping by timestamp and user_id\n",
    "if GROUP_USER_INTERACTIONS_BY_TIMESTAMP:\n",
    "    print(\"\\nForm interactions groups (baskets), by timestamp and user_id...\")\n",
    "    interactions_df = interactions_df.groupby([\"timestamp\", \"user_id\"])[\"item_id\"].apply(list)\n",
    "    interactions_df = interactions_df.reset_index()\n",
    "    interactions_df = interactions_df.sort_values(\"timestamp\")\n",
    "    interactions_df = interactions_df.reset_index(drop=True)\n",
    "else:\n",
    "    print(\"\\nInteractions groups (baskets), by timestamp and user_id, skipped\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy interactions dataframe to complete evaluation dataframe\n",
    "_idf = interactions_df.sort_values(\"timestamp\").groupby([\"user_id\"])[\"item_id\"].apply(list).reset_index().copy()\n",
    "if GROUP_USER_INTERACTIONS_BY_TIMESTAMP:\n",
    "    # Group and flatten interactions to create user profiles\n",
    "    evaluation_df[\"profile\"] = evaluation_df[\"user_id\"].apply(\n",
    "        lambda user_id: [\n",
    "            item_id\n",
    "            for row in _idf[_idf[\"user_id\"] == user_id][\"item_id\"]\n",
    "            for interaction in row\n",
    "            for item_id in interaction\n",
    "        ],\n",
    "    )\n",
    "if MAX_PROFILE_SIZE:\n",
    "    # Reduce size of profiles if needed\n",
    "    evaluation_df[\"profile\"] = evaluation_df[\"profile\"].apply(lambda profile: profile[-MAX_PROFILE_SIZE:])\n",
    "\n",
    "# Rename predict column and drop evaluation column\n",
    "evaluation_df.rename(columns={\"item_id\": \"predict\"}, inplace=True)\n",
    "evaluation_df.drop(columns=[\"evaluation\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCreating helpers instances...\")\n",
    "# Creating hashes container for duplicates detection\n",
    "hashes_container = HashesContainer()\n",
    "\n",
    "# Sampling constants\n",
    "print(\"\\nCalculating important values...\")\n",
    "N_USERS = interactions_df[\"user_id\"].nunique()\n",
    "N_ITEMS = len(features)\n",
    "print(f\">> N_USERS = {N_USERS} | N_ITEMS = {N_ITEMS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_triplet_sampling(samples_per_user, hashes_container, desc=None):\n",
    "    interactions = interactions_df.copy()\n",
    "    samples = []\n",
    "    for ui, group in tqdm(interactions.groupby(\"user_id\"), desc=desc):\n",
    "        # Get profile artworks\n",
    "        full_profile = np.hstack(group[\"item_id\"].values).tolist()\n",
    "        full_profile_set = set(full_profile)\n",
    "        n = samples_per_user\n",
    "        while n > 0:\n",
    "            # Sample positive and negative items\n",
    "            pi_index = random.randrange(len(full_profile))\n",
    "            pi = full_profile[pi_index]\n",
    "            # Get profile\n",
    "            if MAX_PROFILE_SIZE:\n",
    "                # \"pi_index + 1\" to include pi in profile\n",
    "                profile = full_profile[max(0, pi_index - MAX_PROFILE_SIZE + 1):pi_index + 1]\n",
    "            else:\n",
    "                profile = list(full_profile)\n",
    "            # (While loop is in the sampling method)\n",
    "            while True:\n",
    "                ni = random.randint(0, N_ITEMS - 1)\n",
    "                if ni not in full_profile_set:\n",
    "                    break\n",
    "            # If conditions are met, hash and enroll triple\n",
    "            if MODE_PROFILE:\n",
    "                triple = (profile, pi, ni)\n",
    "            else:\n",
    "                triple = (ui, pi, ni)\n",
    "            if not hashes_container.enroll(pre_hash(triple, contains_iter=MODE_PROFILE)):\n",
    "                continue\n",
    "            # If not seen, store sample\n",
    "            samples.append((profile, pi, ni, ui))\n",
    "            n -= 1\n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_training = random_triplet_sampling(\n",
    "    np.ceil(TOTAL_SAMPLES_TRAIN / N_USERS),\n",
    "    hashes_container,\n",
    "    desc=\"Random sampling (training)\",\n",
    ")\n",
    "samples_testing = random_triplet_sampling(\n",
    "    np.ceil(TOTAL_SAMPLES_VALID / N_USERS),\n",
    "    hashes_container,\n",
    "    desc=\"Random sampling (testing)\"\n",
    ")\n",
    "\n",
    "assert len(samples_training) >= TOTAL_SAMPLES_TRAIN\n",
    "assert len(samples_testing) >= TOTAL_SAMPLES_VALID\n",
    "\n",
    "# Total collected samples\n",
    "print(f\"Training samples: {len(samples_training)} ({TOTAL_SAMPLES_TRAIN})\")\n",
    "print(f\"Testing samples: {len(samples_testing)} ({TOTAL_SAMPLES_VALID})\")\n",
    "\n",
    "# Log out detected collisions\n",
    "print(f\">> Total hash collisions: {hashes_container.collisions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge triples into a single list\n",
    "print(\"\\nMerging strategies samples into a single list\")\n",
    "TRAINING_DATA = samples_training\n",
    "print(f\">> Training samples: {len(TRAINING_DATA)}\")\n",
    "# Merge strategies samples\n",
    "VALIDATION_DATA = samples_testing\n",
    "print(f\">> Validation samples: {len(VALIDATION_DATA)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for duplicated hashes\n",
    "print(f\"\\nNaive triples validation and looking for duplicates...\")\n",
    "validation_hash_check = HashesContainer()\n",
    "all_samples = [\n",
    "    triple\n",
    "    for subset in (TRAINING_DATA, VALIDATION_DATA)\n",
    "    for triple in subset\n",
    "]\n",
    "user_ids = interactions_df[\"user_id\"].unique()\n",
    "user_data = dict()\n",
    "for triple in tqdm(all_samples, desc=\"Naive validation\"):\n",
    "    profile, pi, ni, ui = triple\n",
    "    if MODE_PROFILE:\n",
    "        assert validation_hash_check.enroll(pre_hash((profile, pi, ni)))\n",
    "    else:\n",
    "        assert validation_hash_check.enroll(pre_hash((ui, pi, ni), contains_iter=False))\n",
    "    assert 0 <= pi < N_ITEMS\n",
    "    assert 0 <= ni < N_ITEMS\n",
    "    assert pi != ni\n",
    "    if ui == -1:\n",
    "        continue\n",
    "    assert ui in user_ids\n",
    "    if not ui in user_data:\n",
    "        user = interactions_df[interactions_df[\"user_id\"] == ui]\n",
    "        user_data[ui] = set(np.hstack(user[\"item_id\"].values))\n",
    "    user_artworks = user_data[ui]\n",
    "    assert all(i in user_artworks for i in profile)\n",
    "print(\">> No duped hashes found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCreating output files (train and valid)...\")\n",
    "# Training dataframe\n",
    "df_train = pd.DataFrame(TRAINING_DATA, columns=[\"profile\", \"pi\", \"ni\", \"ui\"])\n",
    "df_train[\"profile\"] = df_train[\"profile\"].map(lambda l: \" \".join(map(str, l)))\n",
    "print(f\">> Saving training samples ({OUTPUT_TRAIN_PATH})\")\n",
    "df_train.to_csv(OUTPUT_TRAIN_PATH, index=False)\n",
    "\n",
    "# Validation dataframe\n",
    "df_validation = pd.DataFrame(VALIDATION_DATA, columns=[\"profile\", \"pi\", \"ni\", \"ui\"])\n",
    "df_validation[\"profile\"] = df_validation[\"profile\"].map(lambda l: \" \".join(map(str, l)))\n",
    "print(f\">> Saving validation samples ({OUTPUT_VALID_PATH})\")\n",
    "df_validation.to_csv(OUTPUT_VALID_PATH, index=False)\n",
    "\n",
    "# Evaluation dataframe\n",
    "df_evaluation = evaluation_df.copy()\n",
    "# if GROUP_USER_INTERACTIONS_BY_TIMESTAMP:\n",
    "#     df_evaluation[\"predict\"] = df_evaluation[\"predict\"].map(lambda l: \" \".join(map(str, l)))\n",
    "df_evaluation[\"profile\"] = df_evaluation[\"profile\"].map(lambda l: \" \".join(map(str, l)))\n",
    "print(f\">> Saving evaluation data ({OUTPUT_EVAL_PATH})\")\n",
    "df_evaluation.to_csv(OUTPUT_EVAL_PATH, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
