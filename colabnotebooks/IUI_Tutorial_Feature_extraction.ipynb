{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ialab-puc/VisualRecSys-Tutorial-IUI2021/blob/main/colabnotebooks/IUI_Tutorial_Feature_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oaMpjN7K3Fh"
      },
      "source": [
        "# Visual Feature Extraction\n",
        "\n",
        "This notebook is part of the ACM IUI 2021 Tutorial <b>VisRec: A Hands-on Tutorial on Deep Learning for Visual Recommender Systems</b>\n",
        "\n",
        "@authors\n",
        "\n",
        "Felipe Del RÃ­o, PUC Chile <br/>\n",
        "Denis Parra, PUC Chile\n",
        "\n",
        "Check for updates at the official github repository: https://github.com/ialab-puc/VisualRecSys-Tutorial-IUI2021"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-rf2PPxMQw9"
      },
      "source": [
        "## Dataset: Wikimedia Commons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv2fGWgEK422"
      },
      "source": [
        "Let's start downloading image dataset, which comes from Wikimedia Commons. We acknowledge the support of [Diego Saez-Trumper](https://wikimediafoundation.org/profile/diego-saez-trumper/) from Wikimedia foundation to collect this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IUg5r1YcwAB"
      },
      "outputs": [],
      "source": [
        "!gdown 1rXRT4Pa1opD_3koIQ2uvImjBlACcdC6R"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNw5BWT-UHPn"
      },
      "outputs": [],
      "source": [
        "![[ ! -d wikimedia_recsys ]] && unzip -q wikimedia_recsys.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1_aVbRCUqSj"
      },
      "outputs": [],
      "source": [
        "path_to_data = 'wikimedia_recsys/images'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5s11UEENSYV"
      },
      "source": [
        "At this point you should have a folder called **wikimedia_recsys** containing 4969 image files in it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c00xmql5uPF3"
      },
      "outputs": [],
      "source": [
        "!ls wikimedia_recsys/images | wc -l"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdtNClelNQ-h"
      },
      "source": [
        "Next, let's import all the necesary libraries to execute the code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvBZO6tpJU_o"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "from pathlib import Path\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torchvision.io import read_image\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "import PIL\n",
        "from PIL import Image, ImageFile\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrMRjv2vviML"
      },
      "source": [
        "# Read Wikimedia Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIC6qsr_Oyst"
      },
      "outputs": [],
      "source": [
        "# Needed for some images in the Wikimedia dataset,\n",
        "# They are too large for PIL\n",
        "\n",
        "Image.MAX_IMAGE_PIXELS = 3_000_000_000\n",
        "# Some images are \"broken\" in Wikimedia dataset\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPyoRNd8UqaN"
      },
      "outputs": [],
      "source": [
        "# Create an array data[] to store images and images' paths\n",
        "\n",
        "file_types = ['jpg', 'jpeg', 'png']\n",
        "\n",
        "data = defaultdict(list)\n",
        "for file_type in file_types:\n",
        "    for path in tqdm(Path(path_to_data).glob(f'*.{file_type}')):\n",
        "        try:\n",
        "            image = Image.open(str(path)).convert(\"RGB\")\n",
        "            data['image_path'].append(path)\n",
        "            data['images'].append(image)\n",
        "        except RuntimeError:\n",
        "            pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfFuHKoIw9l0"
      },
      "source": [
        "## Sample of Wikimedia Dataset Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSUNX_dlT6Th"
      },
      "source": [
        "Let's see some images from the dataset.\n",
        "\n",
        "We start by defining the function **display_images()** which allows us to see these a list of images within a grid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xO5HwJKOkTEb"
      },
      "outputs": [],
      "source": [
        "def display_images(elements, columns=5):\n",
        "    for label, items in elements.items():\n",
        "        n_rows = ((len(items) - 1) // columns + 1)\n",
        "\n",
        "        fig = plt.figure(figsize=(columns * 2, 2.7 * n_rows))\n",
        "        plt.title(f\"{label.title()} (n={len(items)})\\n\", fontdict={'fontweight': 'bold'})\n",
        "        plt.axis(\"off\")\n",
        "        for i, item in enumerate(items, start=1):\n",
        "            image_title = item.get('title', \"\")\n",
        "            image = item['image']\n",
        "            image_subtitle = item.get('subtitle', \"\")\n",
        "\n",
        "            image_height = image.height\n",
        "            subtext_loc_x = 0\n",
        "            subtext_loc_y = image_height + 30\n",
        "\n",
        "            ax = fig.add_subplot(n_rows, columns, i)\n",
        "            ax.patch.set_edgecolor(\"black\")\n",
        "            ax.patch.set_linewidth(5)\n",
        "            ax.set_title(image_title, color=\"black\")\n",
        "            ax.text(subtext_loc_x, subtext_loc_y, image_subtitle)\n",
        "\n",
        "            plt.xticks([])\n",
        "            plt.yticks([])\n",
        "            plt.imshow(image)\n",
        "\n",
        "to_std_size = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SshdNwBtxNVQ"
      },
      "outputs": [],
      "source": [
        "n_items = 15\n",
        "columns = 5\n",
        "\n",
        "title = \"Random Wikimedia Dataset Images\\n\"\n",
        "\n",
        "selected_indices = random.choices(list(range(len(data['images']))), k=n_items)\n",
        "# items = [(idx, to_std_size(data['images'][idx])) for idx in sorted(selected_indices)]\n",
        "items_to_display = [{\n",
        "    'title': f'idx={idx}',\n",
        "    'image': to_std_size(data['images'][idx])\n",
        "    } for idx in sorted(selected_indices)]\n",
        "\n",
        "display_images({\"Random Wikimedia Dataset Images\": items_to_display}, columns=columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qcop1lWkvojR"
      },
      "source": [
        "# Feature Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVNhpaF_N55F"
      },
      "source": [
        "We define a class **FeatureExtractor** which will allow us to utilize pretrained models to obtain visual features from input images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5fSJi4Yvgjq"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self, base_model, output_layer, requires_grad=True):\n",
        "        super().__init__()\n",
        "        self.output_layer = output_layer\n",
        "        self.pretrained = base_model(pretrained=True)\n",
        "        self.children_list = []\n",
        "        for n, c in self.pretrained.named_children():\n",
        "            self.children_list.append(c)\n",
        "            if n == self.output_layer:\n",
        "                break\n",
        "\n",
        "        self.net = nn.Sequential(*self.children_list)\n",
        "        for param in self.net.parameters():\n",
        "            param.requires_grad = requires_grad\n",
        "\n",
        "        self.pretrained = None\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.net(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgHVHNI1OZYd"
      },
      "source": [
        "The following code will allow us to get crops from images to visualize them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbMK9J4KoQPk"
      },
      "outputs": [],
      "source": [
        "# Mean and std deviation from images in the Imagenet dataset\n",
        "mean_pixel = [0.485, 0.456, 0.406]\n",
        "std_pixel = [0.229, 0.224, 0.225]\n",
        "\n",
        "normalize_image = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean_pixel, std=std_pixel)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgrfBSePvA9t"
      },
      "source": [
        "## AlexNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLXuliTsPy73"
      },
      "source": [
        "Let's start with the initial Deep Convolutional Neural Network, the AlexNet.\n",
        "\n",
        "> Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 1097-1105.\n",
        "\n",
        "\n",
        "\n",
        "Remember  that this was the first Deep CNN who produced a large gain in performance in the ImageNet challenge in 2012."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTST9tSSZuY4"
      },
      "outputs": [],
      "source": [
        "# Load the pretrained model with this simple code!\n",
        "\n",
        "models.alexnet()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfH-DhM4UKo_"
      },
      "outputs": [],
      "source": [
        "# this block downloads a checkpoint of AlexNet from pytorch\n",
        "\n",
        "model = FeatureExtractor(models.alexnet, 'avgpool', requires_grad=False)\n",
        "model = model.cuda()\n",
        "\n",
        "# we use model.eval() to prevent \"training\" mode\n",
        "model.eval()\n",
        "for image in tqdm(data['images']):\n",
        "    normalized_image = normalize_image(image).unsqueeze(dim=0)\n",
        "    normalized_image = normalized_image.cuda()\n",
        "    features = model(normalized_image)\n",
        "    features = F.adaptive_avg_pool2d(features, output_size=1) if len(features.shape) > 2 else features\n",
        "    features = features.cpu()\n",
        "    data['alexnet_features'].append(features)\n",
        "\n",
        "data['alexnet_features'] = torch.cat(data['alexnet_features'])\n",
        "data['alexnet_features'] = data['alexnet_features'].squeeze()\n",
        "\n",
        "model = normalized_image = features = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPPXQ_glwlmf"
      },
      "source": [
        "# Display Similar Images based on DNN Visual Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHDzwwstRd8T"
      },
      "source": [
        "\n",
        "So far we have our images' visual features. What's next? We can use these features to identify similar images. The following method will allow us:\n",
        "\n",
        "*   To find the *k* most similar images to an input image *query*: **topk_similar**(query, knowledge_base, k=10)\n",
        "\n",
        "\n",
        "We will use this function for the already instanced feature extractor (AlexNet), but we will also use it for the other methods (VGG, ResNet, NASNet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVYDbmPoTACa"
      },
      "outputs": [],
      "source": [
        "def topk_similar(query, knowledge_base, k=10):\n",
        "    k = k + 1 # Remove self similarity\n",
        "    distances = 1 - F.cosine_similarity(knowledge_base, query)\n",
        "    topk = distances.topk(k, largest=False)\n",
        "    indices = topk.indices.tolist()\n",
        "    distances = topk.values.tolist()\n",
        "\n",
        "    if len(indices) > 0:\n",
        "        # Remove query item\n",
        "        closest_distance = distances[0]\n",
        "        indices = indices[1:] if math.isclose(closest_distance, 0.0) else indices\n",
        "        distances = distances[1:] if math.isclose(closest_distance, 0.0) else distances\n",
        "\n",
        "    return indices, distances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaTfF1D_TFcF"
      },
      "source": [
        "Let's play with it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzcVKp3JVihf"
      },
      "outputs": [],
      "source": [
        "#pick_randomly = True\n",
        "#idx = random.randint(0, len(data['image_path'])) if pick_randomly else idx\n",
        "\n",
        "\n",
        "idx = 2197\n",
        "\n",
        "features_id = 'alexnet_features'\n",
        "features = data[features_id][idx, :].unsqueeze(0)\n",
        "\n",
        "image = to_std_size(data['images'][idx])\n",
        "\n",
        "closest_indices, closest_distances = topk_similar( features, data[features_id], k=10 )\n",
        "closest_images = [{\n",
        "    'title': f'idx={idx_}',\n",
        "    'image': to_std_size(data['images'][idx_]),\n",
        "    'subtitle': f'similarity={distance:.4f}'\n",
        "    } for idx_, distance in zip(closest_indices, closest_distances)]\n",
        "\n",
        "display_images({\n",
        "    'Query Image': [{\n",
        "        'title': f'idx={idx}',\n",
        "        'image': image\n",
        "        }],\n",
        "    f'Closest Images using AlexNet features': closest_images\n",
        "}, columns=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j08f3x4GXtR-"
      },
      "source": [
        "Let's try this code with other Deep CNNs! VGG_16, ResNet_18 and NASNet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJw8QKRCwDC8"
      },
      "source": [
        "## VGG16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7jD1Jta99pg"
      },
      "source": [
        "The VGG architecture was able to train deeper networks by using smaller filter (notice differences in kernel size with Alexnet)\n",
        "\n",
        "> Simonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_VnewzFZ3Lh"
      },
      "outputs": [],
      "source": [
        "models.vgg16()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4iuzTc8UKrs"
      },
      "outputs": [],
      "source": [
        "# this block downloads a checkpoint of VGG-16 from pytorch\n",
        "\n",
        "model = FeatureExtractor(models.vgg16, 'avgpool', requires_grad=False)\n",
        "model = model.cuda()\n",
        "\n",
        "model.eval()\n",
        "for image in tqdm(data['images']):\n",
        "    normalized_image = normalize_image(image).unsqueeze(dim=0)\n",
        "    normalized_image = normalized_image.cuda()\n",
        "    features = model(normalized_image)\n",
        "    features = F.adaptive_avg_pool2d(features, output_size=1) if len(features.shape) > 2 else features\n",
        "    features = features.cpu()\n",
        "    data['vgg16_features'].append(features)\n",
        "\n",
        "data['vgg16_features'] = torch.cat(data['vgg16_features'])\n",
        "data['vgg16_features'] = data['vgg16_features'].squeeze()\n",
        "\n",
        "model = normalized_image = features = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NJ6LEttv0tD"
      },
      "source": [
        "## ResNet18"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVAnROiUAZZH"
      },
      "source": [
        "ResNet introduces residual connections, what reduces number of parameters and allows to increase network depth\n",
        "\n",
        "> He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptdZsMzuVYiN"
      },
      "outputs": [],
      "source": [
        "models.resnet18()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzN9-cTGUKgU"
      },
      "outputs": [],
      "source": [
        "# this block downloads a checkpoint of ResNet-18 from pytorch\n",
        "\n",
        "model = FeatureExtractor(models.resnet18, 'avgpool', requires_grad=False)\n",
        "model = model.cuda()\n",
        "\n",
        "model.eval()\n",
        "for image in tqdm(data['images']):\n",
        "    normalized_image = normalize_image(image).unsqueeze(dim=0)\n",
        "    normalized_image = normalized_image.cuda()\n",
        "    features = model(normalized_image)\n",
        "    features = F.adaptive_avg_pool2d(features, output_size=1) if len(features.shape) > 2 else features # Reduce feature cube to 1-d\n",
        "    features = features.cpu()\n",
        "    data['resnet18_features'].append(features)\n",
        "\n",
        "data['resnet18_features'] = torch.cat(data['resnet18_features'])\n",
        "data['resnet18_features'] = data['resnet18_features'].squeeze()\n",
        "\n",
        "model = normalized_image = features = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GD8SY3QwMJ9"
      },
      "source": [
        "## MNASNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cot4B1UCBKVH"
      },
      "source": [
        "In NASNet (Network Architecture Search Networks) the global architure is fixed but blocks or cells are not predefined by authors. Instead, they are searched by reinforcement learning search.\n",
        "\n",
        "> Zoph, B., Vasudevan, V., Shlens, J., & Le, Q. V. (2018). Learning transferable architectures for scalable image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 8697-8710).\n",
        "\n",
        "MNASNet is the version available in Pytorch, where \"M\" is for Mobile\n",
        "\n",
        "\n",
        "> Tan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M., Howard, A., & Le, Q. V. (2019). Mnasnet: Platform-aware neural architecture search for mobile. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 2820-2828).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeQrEm-8wJKD"
      },
      "outputs": [],
      "source": [
        "models.mnasnet1_0()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dn8cMtaOwJKV"
      },
      "outputs": [],
      "source": [
        "# this block downloads a checkpoint of NASNet from pytorch\n",
        "\n",
        "model = FeatureExtractor(models.mnasnet1_0, 'layers', requires_grad=False)\n",
        "model = model.cuda()\n",
        "\n",
        "model.eval()\n",
        "for image in tqdm(data['images']):\n",
        "    normalized_image = normalize_image(image).unsqueeze(dim=0)\n",
        "    normalized_image = normalized_image.cuda()\n",
        "    features = model(normalized_image)\n",
        "    features = F.adaptive_avg_pool2d(features, output_size=1) if len(features.shape) > 2 else features\n",
        "    features = features.cpu()\n",
        "    data['mnasnet_features'].append(features)\n",
        "\n",
        "data['mnasnet_features'] = torch.cat(data['mnasnet_features'])\n",
        "data['mnasnet_features'] = data['mnasnet_features'].squeeze()\n",
        "\n",
        "model = normalized_image = features = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4I7Zj_-xCAX"
      },
      "source": [
        "## Closest Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ZV1Cw2dgguMB"
      },
      "outputs": [],
      "source": [
        "idx =  596#@param {type:\"integer\"}\n",
        "pick_randomly = False #@param {type:\"boolean\"}\n",
        "features_to_compare = \"resnet18\" #@param [\"\", \"alexnet\", \"vgg16\", \"resnet18\", \"mnasnet\"]\n",
        "k_closest = 10 #@param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veTC35YRa65A"
      },
      "outputs": [],
      "source": [
        "idx = random.randint(0, len(data['image_path'])) if pick_randomly else idx\n",
        "\n",
        "features_to_compare = random.choice(\n",
        "    [\"alexnet\", \"vgg16\", \"resnet18\", \"mnasnet\"]) if not features_to_compare else features_to_compare\n",
        "features_id = f'{features_to_compare}_features'\n",
        "features = data[features_id][idx, :].unsqueeze(0)\n",
        "\n",
        "image = to_std_size(data['images'][idx])\n",
        "\n",
        "closest_indices, closest_distances = topk_similar(features, data[features_id], k=k_closest)\n",
        "closest_images = [{\n",
        "    'title': f'idx={idx_}',\n",
        "    'image': to_std_size(data['images'][idx_]),\n",
        "    'subtitle': f'similarity={distance:.4f}'\n",
        "    } for idx_, distance in zip(closest_indices, closest_distances)]\n",
        "\n",
        "display_images({\n",
        "    'Query Image': [{\n",
        "        'title': f'idx={idx}',\n",
        "        'image': image\n",
        "        }],\n",
        "    f'Closest Images using {features_to_compare} features': closest_images\n",
        "}, columns=6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO6nLMH-YfSD"
      },
      "source": [
        "## Missing aspects\n",
        "\n",
        "**How to store and use this for a recommendation system?**\n",
        "\n",
        "We can save the visual features as a numpy file and load it in another script or notebook. We will talk about that next."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}